Feature engineering
Preparing training and test sets
Ci sono 19 serie con 256 utenti.
Ci sono 0 serie con 49 utenti.
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_1 (LSTM)                (None, 20, 4)             144       
_________________________________________________________________
dropout_1 (Dropout)          (None, 20, 4)             0         
_________________________________________________________________
lstm_2 (LSTM)                (None, 16)                1344      
_________________________________________________________________
dropout_2 (Dropout)          (None, 16)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 4)                 68        
=================================================================
Total params: 1,556
Trainable params: 1,556
Non-trainable params: 0
_________________________________________________________________
Training model...
Training
2019-10-08 15:34:14.914377: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
WARNING:tensorflow:From /Users/michele/PycharmProjects/varie/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

Epoch 1/100
 - 1s - loss: 0.7374
Epoch 2/100
 - 0s - loss: 0.4592
Epoch 3/100
 - 0s - loss: 0.2244
Epoch 4/100
 - 0s - loss: 0.0784
Epoch 5/100
 - 0s - loss: 0.0806
Epoch 6/100
 - 0s - loss: 0.0625
Epoch 7/100
 - 0s - loss: 0.0457
Epoch 8/100
 - 0s - loss: 0.0564
Epoch 9/100
 - 0s - loss: 0.0406
Epoch 10/100
 - 0s - loss: 0.0351
Epoch 11/100
 - 0s - loss: 0.0392
Epoch 12/100
 - 0s - loss: 0.0584
Epoch 13/100
 - 0s - loss: 0.0362
Epoch 14/100
 - 0s - loss: 0.0334
Epoch 15/100
 - 0s - loss: 0.0255
Epoch 16/100
 - 0s - loss: 0.0174
Epoch 17/100
 - 0s - loss: 0.0185
Epoch 18/100
 - 0s - loss: 0.0229
Epoch 19/100
 - 0s - loss: 0.0126
Epoch 20/100
 - 0s - loss: 0.0239
Epoch 21/100
 - 0s - loss: 0.0196
Epoch 22/100
 - 0s - loss: 0.0224
Epoch 23/100
 - 0s - loss: 0.0160
Epoch 24/100
 - 0s - loss: 0.0172
Epoch 25/100
 - 0s - loss: 0.0147
Epoch 26/100
 - 0s - loss: 0.0139
Epoch 27/100
 - 0s - loss: 0.0170
Epoch 28/100
 - 0s - loss: 0.0070
Epoch 29/100
 - 0s - loss: 0.0145
Epoch 30/100
 - 0s - loss: 0.0120
Epoch 31/100
 - 0s - loss: 0.0106
Epoch 32/100
 - 0s - loss: 0.0255
Epoch 33/100
 - 0s - loss: 0.0097
Epoch 34/100
 - 0s - loss: 0.0190
Epoch 35/100
 - 0s - loss: 0.0113
Epoch 36/100
 - 0s - loss: 0.0161
Epoch 37/100
 - 0s - loss: 0.0163
Epoch 38/100
 - 0s - loss: 0.0126
Epoch 39/100
 - 0s - loss: 0.0082
Epoch 40/100
 - 0s - loss: 0.0103
Epoch 41/100
 - 0s - loss: 0.0106
Epoch 42/100
 - 0s - loss: 0.0070
Epoch 43/100
 - 0s - loss: 0.0073
Epoch 44/100
 - 0s - loss: 0.0071
Epoch 45/100
 - 0s - loss: 0.0113
Epoch 46/100
 - 0s - loss: 0.0074
Epoch 47/100
 - 0s - loss: 0.0136
Epoch 48/100
 - 0s - loss: 0.0115
Epoch 49/100
 - 0s - loss: 0.0071
Epoch 50/100
 - 0s - loss: 0.0053
Epoch 51/100
 - 0s - loss: 0.0100
Epoch 52/100
 - 0s - loss: 0.0118
Epoch 53/100
 - 0s - loss: 0.0101
Epoch 54/100
 - 0s - loss: 0.0111
Epoch 55/100
 - 0s - loss: 0.0120
Epoch 56/100
 - 0s - loss: 0.0087
Epoch 57/100
 - 0s - loss: 0.0075
Epoch 58/100
 - 0s - loss: 0.0081
Epoch 59/100
 - 0s - loss: 0.0121
Epoch 60/100
 - 0s - loss: 0.0126
Epoch 61/100
 - 0s - loss: 0.0095
Epoch 62/100
 - 0s - loss: 0.0089
Epoch 63/100
 - 0s - loss: 0.0067
Epoch 64/100
 - 0s - loss: 0.0152
Epoch 65/100
 - 0s - loss: 0.0087
Epoch 66/100
 - 0s - loss: 0.0057
Epoch 67/100
 - 0s - loss: 0.0090
Epoch 68/100
 - 0s - loss: 0.0092
Epoch 69/100
 - 0s - loss: 0.0042
Epoch 70/100
 - 0s - loss: 0.0083
Epoch 71/100
 - 0s - loss: 0.0066
Epoch 72/100
 - 0s - loss: 0.0052
Epoch 73/100
 - 0s - loss: 0.0056
Epoch 74/100
 - 0s - loss: 0.0067
Epoch 75/100
 - 0s - loss: 0.0133
Epoch 76/100
 - 0s - loss: 0.0098
Epoch 77/100
 - 0s - loss: 0.0095
Epoch 78/100
 - 0s - loss: 0.0099
Epoch 79/100
 - 0s - loss: 0.0049
Epoch 80/100
 - 0s - loss: 0.0075
Epoch 81/100
 - 0s - loss: 0.0069
Epoch 82/100
 - 0s - loss: 0.0075
Epoch 83/100
 - 0s - loss: 0.0078
Epoch 84/100
 - 0s - loss: 0.0061
Epoch 85/100
 - 0s - loss: 0.0065
Epoch 86/100
 - 0s - loss: 0.0096
Epoch 87/100
 - 0s - loss: 0.0048
Epoch 88/100
 - 0s - loss: 0.0055
Epoch 89/100
 - 0s - loss: 0.0068
Epoch 90/100
 - 0s - loss: 0.0065
Epoch 91/100
 - 0s - loss: 0.0065
Epoch 92/100
 - 0s - loss: 0.0141
Epoch 93/100
 - 0s - loss: 0.0091
Epoch 94/100
 - 0s - loss: 0.0079
Epoch 95/100
 - 0s - loss: 0.0059
Epoch 96/100
 - 0s - loss: 0.0068
Epoch 97/100
 - 0s - loss: 0.0046
Epoch 98/100
 - 0s - loss: 0.0092
Epoch 99/100
 - 0s - loss: 0.0040
Epoch 100/100
 - 0s - loss: 0.0083
Training duration (s) : %s 3.7833480834960938
Training Loss per epoch: [0.7373818159103394, 0.4592133164405823, 0.224419966340065, 0.07844620943069458, 0.08056478947401047, 0.062467582523822784, 0.04568752273917198, 0.05640646070241928, 0.040581196546554565, 0.035113491117954254, 0.039201728999614716, 0.0583769790828228, 0.03619901463389397, 0.033410049974918365, 0.02551041543483734, 0.01741824485361576, 0.01846812479197979, 0.02293640933930874, 0.012574455700814724, 0.023882132023572922, 0.01960793323814869, 0.022397173568606377, 0.01600588671863079, 0.017192983999848366, 0.014669110998511314, 0.013887590728700161, 0.017006494104862213, 0.007036236580461264, 0.014475119300186634, 0.01203839760273695, 0.010557493194937706, 0.025519365444779396, 0.009652662090957165, 0.019019359722733498, 0.011326783336699009, 0.01607932336628437, 0.01633240468800068, 0.012586508877575397, 0.008192804642021656, 0.010339122265577316, 0.010553999803960323, 0.006964647676795721, 0.007279291749000549, 0.007144233677536249, 0.011291354894638062, 0.007439615670591593, 0.01360783725976944, 0.011458431370556355, 0.007054366637021303, 0.005273945163935423, 0.009956379421055317, 0.011808321811258793, 0.010134982876479626, 0.011074529960751534, 0.011992678977549076, 0.008721331134438515, 0.007497658021748066, 0.008088578470051289, 0.012096904218196869, 0.012600195594131947, 0.009506888687610626, 0.00888684019446373, 0.006674285978078842, 0.015246422961354256, 0.00872273463755846, 0.005662144627422094, 0.008969037793576717, 0.00923068542033434, 0.004159844480454922, 0.008345779962837696, 0.006554830819368362, 0.005238729994744062, 0.005594747606664896, 0.006699918303638697, 0.013275079429149628, 0.009835454635322094, 0.00947762280702591, 0.00985370296984911, 0.0049192169681191444, 0.007528810296207666, 0.006905305664986372, 0.007533211261034012, 0.007780658546835184, 0.006069717463105917, 0.0065277935937047005, 0.009588546119630337, 0.0047684418968856335, 0.005463209003210068, 0.006760026328265667, 0.00650781998410821, 0.006451308727264404, 0.014111064374446869, 0.009122567251324654, 0.007920988835394382, 0.00589608121663332, 0.006793058943003416, 0.004606288392096758, 0.009175518527626991, 0.003969056066125631, 0.008322772569954395]
dict_keys(['loss'])
Training done.
PyDev console: starting.

Python 3.6.3 (v3.6.3:2c5fed86e0, Oct  3 2017, 00:32:08) 
[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin
