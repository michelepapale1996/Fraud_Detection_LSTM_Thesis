Preparing training and test sets
Ci sono 7811 serie con 57228 utenti.
Ci sono 104 serie con 23151 utenti.
Creating the model...
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_1 (LSTM)                (None, 20, 50)            11000     
_________________________________________________________________
lstm_2 (LSTM)                (None, 50)                20200     
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 51        
=================================================================
Total params: 31,251
Trainable params: 31,251
Non-trainable params: 0
_________________________________________________________________
Training model...
Training
2019-10-09 14:44:35.319448: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
WARNING:tensorflow:From /Users/michele/PycharmProjects/varie/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

Epoch 1/100
 - 5s - loss: 0.0122
Epoch 2/100
 - 4s - loss: 0.0115
Epoch 3/100
 - 4s - loss: 0.0114
Epoch 4/100
 - 4s - loss: 0.0115
Epoch 5/100
 - 4s - loss: 0.0114
Epoch 6/100
 - 4s - loss: 0.0114
Epoch 7/100
 - 4s - loss: 0.0115
Epoch 8/100
 - 4s - loss: 0.0115
Epoch 9/100
 - 4s - loss: 0.0115
Epoch 10/100
 - 4s - loss: 0.0114
Epoch 11/100
 - 4s - loss: 0.0115
Epoch 12/100
 - 4s - loss: 0.0114
Epoch 13/100
 - 4s - loss: 0.0114
Epoch 14/100
 - 4s - loss: 0.0115
Epoch 15/100
 - 4s - loss: 0.0115
Epoch 16/100
 - 4s - loss: 0.0115
Epoch 17/100
 - 4s - loss: 0.0115
Epoch 18/100
 - 4s - loss: 0.0115
Epoch 19/100
 - 4s - loss: 0.0115
Epoch 20/100
 - 4s - loss: 0.0115
Epoch 21/100
 - 4s - loss: 0.0115
Epoch 22/100
 - 4s - loss: 0.0115
Epoch 23/100
 - 4s - loss: 0.0115
Epoch 24/100
 - 4s - loss: 0.0115
Epoch 25/100
 - 4s - loss: 0.0115
Epoch 26/100
 - 4s - loss: 0.0115
Epoch 27/100
 - 4s - loss: 0.0115
Epoch 28/100
 - 4s - loss: 0.0115
Epoch 29/100
 - 4s - loss: 0.0115
Epoch 30/100
 - 4s - loss: 0.0115
Epoch 31/100
 - 4s - loss: 0.0115
Epoch 32/100
 - 4s - loss: 0.0115
Epoch 33/100
 - 4s - loss: 0.0115
Epoch 34/100
 - 4s - loss: 0.0115
Epoch 35/100
 - 4s - loss: 0.0115
Epoch 36/100
 - 4s - loss: 0.0115
Epoch 37/100
 - 4s - loss: 0.0115
Epoch 38/100
 - 4s - loss: 0.0115
Epoch 39/100
 - 4s - loss: 0.0115
Epoch 40/100
 - 4s - loss: 0.0115
Epoch 41/100
 - 4s - loss: 0.0115
Epoch 42/100
 - 4s - loss: 0.0115
Epoch 43/100
 - 4s - loss: 0.0115
Epoch 44/100
 - 4s - loss: 0.0115
Epoch 45/100
 - 4s - loss: 0.0115
Epoch 46/100
 - 4s - loss: 0.0115
Epoch 47/100
 - 4s - loss: 0.0115
Epoch 48/100
 - 4s - loss: 0.0115
Epoch 49/100
 - 4s - loss: 0.0115
Epoch 50/100
 - 4s - loss: 0.0115
Epoch 51/100
 - 4s - loss: 0.0115
Epoch 52/100
 - 4s - loss: 0.0115
Epoch 53/100
 - 4s - loss: 0.0115
Epoch 54/100
 - 4s - loss: 0.0115
Epoch 55/100
 - 4s - loss: 0.0115
Epoch 56/100
 - 4s - loss: 0.0115
Epoch 57/100
 - 4s - loss: 0.0115
Epoch 58/100
 - 4s - loss: 0.0115
Epoch 59/100
 - 4s - loss: 0.0115
Epoch 60/100
 - 4s - loss: 0.0115
Epoch 61/100
 - 4s - loss: 0.0115
Epoch 62/100
 - 4s - loss: 0.0115
Epoch 63/100
 - 4s - loss: 0.0115
Epoch 64/100
 - 4s - loss: 0.0115
Epoch 65/100
 - 4s - loss: 0.0115
Epoch 66/100
 - 4s - loss: 0.0115
Epoch 67/100
 - 4s - loss: 0.0115
Epoch 68/100
 - 4s - loss: 0.0115
Epoch 69/100
 - 4s - loss: 0.0115
Epoch 70/100
 - 4s - loss: 0.0115
Epoch 71/100
 - 4s - loss: 0.0115
Epoch 72/100
 - 4s - loss: 0.0115
Epoch 73/100
 - 4s - loss: 0.0115
Epoch 74/100
 - 4s - loss: 0.0115
Epoch 75/100
 - 4s - loss: 0.0115
Epoch 76/100
 - 4s - loss: 0.0115
Epoch 77/100
 - 4s - loss: 0.0115
Epoch 78/100
 - 4s - loss: 0.0115
Epoch 79/100
 - 4s - loss: 0.0115
Epoch 80/100
 - 4s - loss: 0.0115
Epoch 81/100
 - 4s - loss: 0.0115
Epoch 82/100
 - 4s - loss: 0.0115
Epoch 83/100
 - 4s - loss: 0.0115
Epoch 84/100
 - 4s - loss: 0.0115
Epoch 85/100
 - 4s - loss: 0.0115
Epoch 86/100
 - 4s - loss: 0.0115
Epoch 87/100
 - 4s - loss: 0.0115
Epoch 88/100
 - 4s - loss: 0.0115
Epoch 89/100
 - 4s - loss: 0.0115
Epoch 90/100
 - 4s - loss: 0.0115
Epoch 91/100
 - 4s - loss: 0.0115
Epoch 92/100
 - 4s - loss: 0.0115
Epoch 93/100
 - 4s - loss: 0.0115
Epoch 94/100
 - 4s - loss: 0.0115
Epoch 95/100
 - 4s - loss: 0.0115
Epoch 96/100
 - 4s - loss: 0.0115
Epoch 97/100
 - 4s - loss: 0.0115
Epoch 98/100
 - 4s - loss: 0.0115
Epoch 99/100
 - 4s - loss: 0.0115
Epoch 100/100
 - 4s - loss: 0.0115
Training duration (s) : %s 381.10412192344666
Training Loss per epoch: [0.012203688990374644, 0.011481393300788621, 0.01144001755102549, 0.011497166016850125, 0.01142745075523898, 0.011439108144498942, 0.011466094030404394, 0.011470334444754727, 0.011468852697600831, 0.01144515909584803, 0.011453076579661308, 0.011445705794627305, 0.011441797719847114, 0.011501020073954904, 0.011521946010055678, 0.011521913665367546, 0.011521866097182696, 0.01152179014950242, 0.011521649568808206, 0.011521308186635958, 0.011518622078389143, 0.011495067181309668, 0.011522178629418142, 0.011522177225430742, 0.011522176569214237, 0.011522175302650088, 0.011522173974981068, 0.011522172418480834, 0.011522170129537249, 0.011522167092690107, 0.011522163979754415, 0.011522159172864444, 0.011522152703011808, 0.011522143440699278, 0.011522128700980218, 0.011522102532576711, 0.011522039952229597, 0.011521684581102025, 0.011518865370172774, 0.011522198132668544, 0.011522197629048983, 0.011522196469186726, 0.011522195950332458, 0.011522194790486794, 0.01152219337122002, 0.011522191463589359, 0.01152218889597166, 0.011522184642032718, 0.011522178019033163, 0.011522165108773498, 0.011522131096728264, 0.011529554008620147, 0.01152221220370849, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212268570262, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833, 0.011522212264754833]
dict_keys(['loss'])
Training done.
PyDev console: starting.

Python 3.6.3 (v3.6.3:2c5fed86e0, Oct  3 2017, 00:32:08) 
[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin
 32/104 [========>.....................] - ETA: 0s104/104 [==============================] - 0s 1ms/step
0.038461538461538464
