Feature engineering
Preparing training and test sets
Ci sono 7848 serie con 57264 utenti.
Ci sono 98 serie con 23086 utenti.
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lstm_1 (LSTM)                (None, 20, 4)             144       
_________________________________________________________________
dropout_1 (Dropout)          (None, 20, 4)             0         
_________________________________________________________________
lstm_2 (LSTM)                (None, 16)                1344      
_________________________________________________________________
dropout_2 (Dropout)          (None, 16)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 4)                 68        
=================================================================
Total params: 1,556
Trainable params: 1,556
Non-trainable params: 0
_________________________________________________________________
Training model...
Training
2019-10-08 15:40:41.005742: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
WARNING:tensorflow:From /Users/michele/PycharmProjects/varie/venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

Epoch 1/100
 - 4s - loss: 0.0301
Epoch 2/100
 - 3s - loss: 0.0126
Epoch 3/100
 - 3s - loss: 0.0112
Epoch 4/100
 - 3s - loss: 0.0105
Epoch 5/100
 - 3s - loss: 0.0101
Epoch 6/100
 - 3s - loss: 0.0098
Epoch 7/100
 - 3s - loss: 0.0093
Epoch 8/100
 - 3s - loss: 0.0091
Epoch 9/100
 - 3s - loss: 0.0089
Epoch 10/100
 - 3s - loss: 0.0088
Epoch 11/100
 - 3s - loss: 0.0087
Epoch 12/100
 - 3s - loss: 0.0085
Epoch 13/100
 - 3s - loss: 0.0086
Epoch 14/100
 - 3s - loss: 0.0085
Epoch 15/100
 - 3s - loss: 0.0084
Epoch 16/100
 - 3s - loss: 0.0087
Epoch 17/100
 - 3s - loss: 0.0085
Epoch 18/100
 - 3s - loss: 0.0083
Epoch 19/100
 - 3s - loss: 0.0083
Epoch 20/100
 - 3s - loss: 0.0082
Epoch 21/100
 - 3s - loss: 0.0082
Epoch 22/100
 - 3s - loss: 0.0082
Epoch 23/100
 - 3s - loss: 0.0083
Epoch 24/100
 - 3s - loss: 0.0081
Epoch 25/100
 - 3s - loss: 0.0082
Epoch 26/100
 - 3s - loss: 0.0084
Epoch 27/100
 - 3s - loss: 0.0081
Epoch 28/100
 - 3s - loss: 0.0081
Epoch 29/100
 - 4s - loss: 0.0083
Epoch 30/100
 - 3s - loss: 0.0081
Epoch 31/100
 - 4s - loss: 0.0079
Epoch 32/100
 - 4s - loss: 0.0079
Epoch 33/100
 - 3s - loss: 0.0081
Epoch 34/100
 - 4s - loss: 0.0080
Epoch 35/100
 - 4s - loss: 0.0080
Epoch 36/100
 - 3s - loss: 0.0080
Epoch 37/100
 - 4s - loss: 0.0080
Epoch 38/100
 - 3s - loss: 0.0081
Epoch 39/100
 - 3s - loss: 0.0081
Epoch 40/100
 - 4s - loss: 0.0079
Epoch 41/100
 - 4s - loss: 0.0081
Epoch 42/100
 - 3s - loss: 0.0080
Epoch 43/100
 - 3s - loss: 0.0081
Epoch 44/100
 - 3s - loss: 0.0079
Epoch 45/100
 - 3s - loss: 0.0080
Epoch 46/100
 - 3s - loss: 0.0080
Epoch 47/100
 - 3s - loss: 0.0078
Epoch 48/100
 - 3s - loss: 0.0078
Epoch 49/100
 - 3s - loss: 0.0080
Epoch 50/100
 - 3s - loss: 0.0078
Epoch 51/100
 - 3s - loss: 0.0079
Epoch 52/100
 - 3s - loss: 0.0079
Epoch 53/100
 - 3s - loss: 0.0079
Epoch 54/100
 - 3s - loss: 0.0080
Epoch 55/100
 - 3s - loss: 0.0078
Epoch 56/100
 - 3s - loss: 0.0079
Epoch 57/100
 - 3s - loss: 0.0078
Epoch 58/100
 - 3s - loss: 0.0081
Epoch 59/100
 - 3s - loss: 0.0079
Epoch 60/100
 - 3s - loss: 0.0078
Epoch 61/100
 - 3s - loss: 0.0080
Epoch 62/100
 - 3s - loss: 0.0080
Epoch 63/100
 - 3s - loss: 0.0078
Epoch 64/100
 - 3s - loss: 0.0079
Epoch 65/100
 - 3s - loss: 0.0079
Epoch 66/100
 - 3s - loss: 0.0078
Epoch 67/100
 - 3s - loss: 0.0080
Epoch 68/100
 - 3s - loss: 0.0078
Epoch 69/100
 - 3s - loss: 0.0077
Epoch 70/100
 - 3s - loss: 0.0085
Epoch 71/100
 - 3s - loss: 0.0078
Epoch 72/100
 - 3s - loss: 0.0081
Epoch 73/100
 - 3s - loss: 0.0079
Epoch 74/100
 - 3s - loss: 0.0081
Epoch 75/100
 - 3s - loss: 0.0078
Epoch 76/100
 - 3s - loss: 0.0079
Epoch 77/100
 - 3s - loss: 0.0080
Epoch 78/100
 - 3s - loss: 0.0079
Epoch 79/100
 - 3s - loss: 0.0078
Epoch 80/100
 - 3s - loss: 0.0078
Epoch 81/100
 - 3s - loss: 0.0081
Epoch 82/100
 - 3s - loss: 0.0080
Epoch 83/100
 - 3s - loss: 0.0078
Epoch 84/100
 - 3s - loss: 0.0078
Epoch 85/100
 - 3s - loss: 0.0079
Epoch 86/100
 - 3s - loss: 0.0079
Epoch 87/100
 - 3s - loss: 0.0078
Epoch 88/100
 - 3s - loss: 0.0078
Epoch 89/100
 - 3s - loss: 0.0077
Epoch 90/100
 - 3s - loss: 0.0079
Epoch 91/100
 - 3s - loss: 0.0077
Epoch 92/100
 - 3s - loss: 0.0077
Epoch 93/100
 - 3s - loss: 0.0077
Epoch 94/100
 - 3s - loss: 0.0079
Epoch 95/100
 - 3s - loss: 0.0079
Epoch 96/100
 - 3s - loss: 0.0077
Epoch 97/100
 - 3s - loss: 0.0078
Epoch 98/100
 - 3s - loss: 0.0077
Epoch 99/100
 - 3s - loss: 0.0077
Epoch 100/100
 - 3s - loss: 0.0076
Training duration (s) : %s 307.60157585144043
Training Loss per epoch: [0.0300659064562359, 0.012635230794966706, 0.01121689755565446, 0.010532273953660912, 0.010058897970838587, 0.009833722150453559, 0.009346150887477858, 0.009101647822657233, 0.008933743915404786, 0.008834473966268649, 0.008749207294243222, 0.00847330554870264, 0.0086214286287075, 0.008500445932937856, 0.008403112845617336, 0.008734824718569503, 0.008470325952832028, 0.008338904912238437, 0.008295123021533214, 0.00820380419063848, 0.008194999166723496, 0.008215406129863417, 0.008319205848835257, 0.008130095358444311, 0.008199616548463809, 0.00836309147941729, 0.00805291969606332, 0.008135506273299516, 0.008250166284903944, 0.008072339365008181, 0.007891207549646952, 0.007870855426489852, 0.008129511704710175, 0.007961916237182832, 0.007983153867967252, 0.007952279490527908, 0.007983700398148762, 0.008126074046648423, 0.008066003140945948, 0.007947900007354526, 0.008069509060276819, 0.008043129057371865, 0.008056160755109927, 0.007902706082042583, 0.007996279845976865, 0.008019133545951984, 0.007846195819891162, 0.007828209510264836, 0.007955741192151058, 0.007788223456415736, 0.007903353650755155, 0.00793804278716855, 0.007871728185868074, 0.0080027828998408, 0.007814576895148032, 0.007891265915613717, 0.007784128257440864, 0.008113508898858361, 0.00791506234923839, 0.007761910606876289, 0.008046598827787252, 0.008043741748837239, 0.007771913528254409, 0.00791179232900273, 0.007921902256176182, 0.00779590510623672, 0.007951309656910408, 0.007793590727629601, 0.0077466404456626265, 0.008516527791142038, 0.007833970792754863, 0.008124547352146634, 0.007857135625790603, 0.008064133982353528, 0.007803731491631628, 0.007903802036293511, 0.008044371423767224, 0.007867851350055372, 0.007827325251283813, 0.007823216246353666, 0.00811871417749922, 0.008017725198720483, 0.007762323897763912, 0.007806386829949118, 0.007912421780833016, 0.007938541961116118, 0.00775087633130789, 0.007766565621026415, 0.007656236476322805, 0.00793081225312783, 0.007732026283719428, 0.007745720365414136, 0.0076724583746829314, 0.007927466792440864, 0.007946968563973797, 0.007707068653778191, 0.0078043334751421245, 0.007701784108526184, 0.0076962552483361615, 0.007633716627009873]
dict_keys(['loss'])
Training done.
PyDev console: starting.

Python 3.6.3 (v3.6.3:2c5fed86e0, Oct  3 2017, 00:32:08) 
[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin
